# scripts/search_api_improved_final.py
import json
import pickle
import numpy as np
import sys
import os
import re
from collections import Counter

# Th√™m current directory v√†o Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.user_manager import UserManager
except ImportError:
    # Fallback import
    import importlib.util
    spec = importlib.util.spec_from_file_location("user_manager", "user_manager.py")
    user_manager = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(user_manager)
    UserManager = user_manager.UserManager

class ImprovedFinalSearchAPI:
    def __init__(self):
        self.user_mgr = UserManager()
        self.vector_store = self._load_improved_vector_store()
        print("üöÄ ƒê√£ kh·ªüi t·∫°o Improved Final Search API")
    
    def _load_improved_vector_store(self):
        """T·∫£i Improved Vector Store - t·ªët h∆°n Fixed"""
        try:
            with open('./improved_vector_store/vector_store.pkl', 'rb') as f:
                data = pickle.load(f)
            print("‚úÖ ƒê√£ t·∫£i Improved Vector Store")
            print(f"   ‚Ä¢ Documents: {len(data['vectors'])}")
            print(f"   ‚Ä¢ Vector dimension: {data.get('vector_dim', 300)}")
            print(f"   ‚Ä¢ Vocabulary size: {len(data.get('vocab', {}))}")
            return data
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i improved vector store: {e}")
            return {'vectors': {}, 'metadata': {}, 'vocab': {}}
    
    def improved_preprocess(self, text):
        """Ti·ªÅn x·ª≠ l√Ω GI·ªêNG ImprovedVectorStore"""
        # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        text = text.lower().strip()
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë v√† kho·∫£ng tr·∫Øng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Thay th·∫ø nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng m·ªôt kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def create_improved_embedding(self, text):
        """T·∫°o embedding GI·ªêNG ImprovedVectorStore"""
        processed_text = self.improved_preprocess(text)
        words = processed_text.split()
        
        vector_dim = self.vector_store.get('vector_dim', 300)
        vocab = self.vector_store.get('vocab', {})
        
        vector = np.zeros(vector_dim)
        
        if not words:
            return vector
        
        # T√≠nh TF
        word_count = len(words)
        word_freq = Counter(words)
        
        # TF-IDF embedding GI·ªêNG ImprovedVectorStore
        for word, count in word_freq.items():
            if word in vocab:
                word_info = vocab[word]
                tf = count / word_count
                tf_idf = tf * word_info.get('idf', 1.0)
                
                # S·ª≠ d·ª•ng multiple hash functions GI·ªêNG ImprovedVectorStore
                for seed in range(3):
                    hash_val = (hash(word + str(seed)) % (vector_dim // 3)) + (seed * (vector_dim // 3))
                    if hash_val < vector_dim:
                        vector[hash_val] += tf_idf
        
        # Th√™m semantic boost cho c√°c t·ª´ kh√≥a quan tr·ªçng
        important_keywords = {
            'ngh·ªâ ph√©p': [0.8, 0.6, 0.4],
            'l∆∞∆°ng th∆∞·ªüng': [0.7, 0.5, 0.3], 
            'b·∫£o hi·ªÉm': [0.6, 0.4, 0.2],
            'gi·ªù l√†m vi·ªác': [0.5, 0.3, 0.1],
            'h·ª£p ƒë·ªìng': [0.4, 0.2, 0.1],
            'c∆° c·∫•u': [0.3, 0.2, 0.1],
            'h·ªá th·ªëng': [0.3, 0.2, 0.1]
        }
        
        for phrase, weights in important_keywords.items():
            if phrase in processed_text:
                for i, weight in enumerate(weights):
                    if i * 3 < vector_dim:
                        vector[i * 3] += weight
        
        # Chu·∫©n h√≥a vector
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            
        return vector
    
    def cosine_similarity(self, vec1, vec2):
        """Cosine similarity"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return max(0.0, similarity)
    
    def search(self, user_id, query, top_k=5, similarity_threshold=0.1):
        """T√¨m ki·∫øm v·ªõi Improved Vector Store"""
        print(f"üîç User {user_id} t√¨m ki·∫øm: '{query}'")
        
        # Ki·ªÉm tra user permissions
        user_permissions = self.user_mgr.get_user_permissions(user_id)
        if not user_permissions:
            return {"error": "User kh√¥ng t·ªìn t·∫°i", "results": []}
        
        print(f"   Role: {user_permissions['role']}")
        print(f"   Allowed categories: {user_permissions['allowed_categories']}")
        
        # T·∫°o embedding cho query
        query_embedding = self.create_improved_embedding(query)
        
        # T√¨m ki·∫øm
        similarities = []
        for chunk_id, vector in self.vector_store['vectors'].items():
            # Chuy·ªÉn ƒë·ªïi vector sang numpy array n·∫øu c·∫ßn
            if isinstance(vector, list):
                vector = np.array(vector)
            
            similarity = self.cosine_similarity(query_embedding, vector)
            metadata = self.vector_store['metadata'][chunk_id]
            
            # Ki·ªÉm tra ph√¢n quy·ªÅn
            if metadata['category'] in user_permissions['allowed_categories']:
                if similarity >= similarity_threshold:
                    similarities.append((chunk_id, similarity, metadata))
        
        # S·∫Øp x·∫øp v√† l·∫•y k·∫øt qu·∫£
        similarities.sort(key=lambda x: x[1], reverse=True)
        final_results = similarities[:top_k]
        
        print(f"   ‚úÖ T√¨m th·∫•y {len(similarities)} k·∫øt qu·∫£, tr·∫£ v·ªÅ {len(final_results)}")
        print(f"   üìä Similarity range: {[f'{s[1]:.4f}' for s in final_results]}")
        
        # Format k·∫øt qu·∫£
        formatted_results = []
        for chunk_id, similarity, metadata in final_results:
            formatted_results.append({
                'id': chunk_id,
                'content': metadata.get('content', ''),
                'metadata': metadata,
                'similarity': similarity
            })
        
        return {
            "user_info": {
                "user_id": user_id,
                "username": user_permissions['username'],
                "role": user_permissions['role']
            },
            "query": query,
            "total_found": len(similarities),
            "allowed_categories": user_permissions['allowed_categories'],
            "results": formatted_results
        }
    
    def smart_search(self, user_id, query, top_k=5):
        """Smart search v·ªõi threshold th·∫•p h∆°n"""
        return self.search(user_id, query, top_k, similarity_threshold=0.05)

def test_improved_search():
    """Test improved search API"""
    print("üöÄ TEST IMPROVED FINAL SEARCH API")
    print("=" * 60)
    
    api = ImprovedFinalSearchAPI()
    
    test_cases = [
        ('user001', 'ngh·ªâ ph√©p', 'Employee h·ªèi ngh·ªâ ph√©p'),
        ('user001', 'l∆∞∆°ng th∆∞·ªüng', 'Employee h·ªèi l∆∞∆°ng'),
        ('user003', 'l∆∞∆°ng th√°ng 13', 'Manager h·ªèi th∆∞·ªüng'),
        ('user005', 'b·∫£o hi·ªÉm', 'HR h·ªèi b·∫£o hi·ªÉm'),
        ('user001', 'gi·ªù l√†m vi·ªác', 'Employee h·ªèi gi·ªù l√†m'),
        ('user003', 'ch√≠nh s√°ch c√¥ng ty', 'Manager h·ªèi policy'),
        ('user005', 'c∆° c·∫•u l∆∞∆°ng', 'HR h·ªèi l∆∞∆°ng')
    ]
    
    for user_id, query, description in test_cases:
        print(f"\nüéØ {description}")
        print("-" * 40)
        
        result = api.smart_search(user_id, query, top_k=3)
        
        if 'error' in result:
            print(f"‚ùå L·ªói: {result['error']}")
            continue
        
        print(f"üë§ User: {result['user_info']['username']} ({result['user_info']['role']})")
        print(f"üîç Query: '{result['query']}'")
        print(f"üìä Found: {result['total_found']} results")
        
        if result['results']:
            for i, item in enumerate(result['results']):
                print(f"   {i+1}. {item['metadata']['title']} (similarity: {item['similarity']:.4f})")
                print(f"      Category: {item['metadata']['category']}")
                print(f"      Content: {item['content'][:80]}...")
        else:
            print("   ‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ph√π h·ª£p")
    
    print(f"\nüéâ HO√ÄN TH√ÄNH TEST IMPROVED FINAL SEARCH API")

if __name__ == "__main__":
    test_improved_search()