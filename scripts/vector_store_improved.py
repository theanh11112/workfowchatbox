# scripts/vector_store_improved.py
import json
import os
import sys
import numpy as np
from collections import Counter
import re
import math

class ImprovedVectorStore:
    def __init__(self, persist_directory="./improved_vector_store"):
        """Vector store ƒë∆∞·ª£c c·∫£i ti·∫øn v·ªõi embedding t·ªët h∆°n"""
        self.persist_directory = persist_directory
        os.makedirs(persist_directory, exist_ok=True)
        self.vectors = {}
        self.metadata = {}
        self.vocab = {}
        self.vocab_size = 1000
        self.vector_dim = 300  # TƒÉng dimension ƒë·ªÉ capture nhi·ªÅu th√¥ng tin h∆°n
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o Improved Vector Store")
    
    def preprocess_text(self, text):
        """Ti·ªÅn x·ª≠ l√Ω text t·ªët h∆°n"""
        # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        text = text.lower()
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë v√† kho·∫£ng tr·∫Øng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Thay th·∫ø nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng m·ªôt kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def build_vocabulary(self, chunks):
        """X√¢y d·ª±ng vocabulary t·ª´ t·∫•t c·∫£ documents"""
        print("üìö ƒêang x√¢y d·ª±ng vocabulary...")
        
        all_text = " ".join([chunk['content'] for chunk in chunks])
        processed_text = self.preprocess_text(all_text)
        words = processed_text.split()
        
        # ƒê·∫øm t·∫ßn su·∫•t t·ª´
        word_freq = Counter(words)
        
        # L·∫•y c√°c t·ª´ ph·ªï bi·∫øn nh·∫•t
        most_common = word_freq.most_common(self.vocab_size)
        
        # T·∫°o vocabulary v·ªõi TF-IDF weights
        total_docs = len(chunks)
        doc_freq = {}
        
        # T√≠nh document frequency cho m·ªói t·ª´
        for chunk in chunks:
            chunk_words = set(self.preprocess_text(chunk['content']).split())
            for word in chunk_words:
                doc_freq[word] = doc_freq.get(word, 0) + 1
        
        # T·∫°o vocab v·ªõi id v√† weights
        self.vocab = {}
        for idx, (word, freq) in enumerate(most_common):
            # T√≠nh IDF weight
            idf = math.log(total_docs / (doc_freq.get(word, 1) + 1)) + 1
            self.vocab[word] = {
                'id': idx,
                'freq': freq,
                'idf': idf
            }
        
        print(f"‚úÖ ƒê√£ x√¢y d·ª±ng vocabulary v·ªõi {len(self.vocab)} t·ª´")
    
    def create_improved_embedding(self, text):
        """T·∫°o embedding c·∫£i ti·∫øn s·ª≠ d·ª•ng TF-IDF"""
        processed_text = self.preprocess_text(text)
        words = processed_text.split()
        
        # Kh·ªüi t·∫°o vector
        vector = np.zeros(self.vector_dim)
        
        # T√≠nh term frequency
        word_count = len(words)
        if word_count == 0:
            return vector.tolist()
        
        word_freq = Counter(words)
        
        # T·∫°o embedding s·ª≠ d·ª•ng TF-IDF
        for word, count in word_freq.items():
            if word in self.vocab:
                word_info = self.vocab[word]
                # TF-IDF weight
                tf = count / word_count
                tf_idf = tf * word_info['idf']
                
                # S·ª≠ d·ª•ng multiple hash functions ƒë·ªÉ ph√¢n ph·ªëi t·ªët h∆°n
                for seed in range(3):  # 3 hash functions kh√°c nhau
                    hash_val = (hash(word + str(seed)) % (self.vector_dim // 3)) + (seed * (self.vector_dim // 3))
                    vector[hash_val] += tf_idf
        
        # Th√™m bi·ªÉu di·ªÖn cho c·ª•m t·ª´ th√¥ng d·ª•ng
        common_phrases = {
            'ngh·ªâ ph√©p': [0.8, 0.6, 0.4],
            'l∆∞∆°ng th∆∞·ªüng': [0.7, 0.5, 0.3],
            'b·∫£o hi·ªÉm': [0.6, 0.4, 0.2],
            'gi·ªù l√†m vi·ªác': [0.5, 0.3, 0.1],
            'h·ª£p ƒë·ªìng': [0.4, 0.2, 0.1]
        }
        
        for phrase, weights in common_phrases.items():
            if phrase in text.lower():
                for i, weight in enumerate(weights):
                    if i * 3 < self.vector_dim:
                        vector[i * 3] += weight
        
        # Chu·∫©n h√≥a vector
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            
        return vector.tolist()
    
    def add_documents(self, chunks):
        """Th√™m documents v√†o vector store"""
        print("üì• ƒêang th√™m documents v√†o vector store...")
        
        # X√¢y d·ª±ng vocabulary tr∆∞·ªõc
        self.build_vocabulary(chunks)
        
        for chunk in chunks:
            chunk_id = chunk['id']
            content = chunk['content']
            
            # T·∫°o embedding c·∫£i ti·∫øn
            embedding = self.create_improved_embedding(content)
            
            # L∆∞u vector v√† metadata
            self.vectors[chunk_id] = embedding
            self.metadata[chunk_id] = {
                "document_id": chunk['document_id'],
                "category": chunk['category'],
                "allowed_roles": chunk['allowed_roles'],
                "title": chunk['title'],
                "content": content[:200] + "..." if len(content) > 200 else content,
                "word_count": chunk['word_count'],
                "full_content": content  # L∆∞u to√†n b·ªô content ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£ t·ªët h∆°n
            }
        
        print(f"‚úÖ ƒê√£ th√™m {len(chunks)} documents")
        
        # Th·ªëng k√™
        categories = {}
        for chunk in chunks:
            cat = chunk['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"üìä Ph√¢n b·ªë theo category:")
        for cat, count in categories.items():
            print(f"   ‚Ä¢ {cat}: {count} chunks")
    
    def enhanced_cosine_similarity(self, vec1, vec2):
        """Cosine similarity ƒë∆∞·ª£c c·∫£i ti·∫øn"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        base_similarity = dot_product / (norm1 * norm2)
        
        # TƒÉng c∆∞·ªùng similarity cho c√°c vector c√≥ c√πng pattern
        if base_similarity > 0.3:  # Ch·ªâ √°p d·ª•ng cho c√°c k·∫øt qu·∫£ kh√° t∆∞∆°ng ƒë·ªìng
            # Th√™m tr·ªçng s·ªë cho c√°c dimensions c√≥ gi√° tr·ªã cao
            significant_dims = (vec1 > 0.1) & (vec2 > 0.1)
            if np.any(significant_dims):
                enhanced_similarity = np.mean((vec1[significant_dims] + vec2[significant_dims]) / 2)
                base_similarity = 0.7 * base_similarity + 0.3 * enhanced_similarity
        
        return min(base_similarity, 1.0)  # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 1
    
    def search(self, query, n_results=5, similarity_threshold=0.1):
        """T√¨m ki·∫øm c·∫£i ti·∫øn v·ªõi threshold"""
        print(f"\nüîç T√åM KI·∫æM: '{query}'")
        
        # T·∫°o embedding cho query
        query_embedding = self.create_improved_embedding(query)
        
        # T√≠nh similarity v·ªõi t·∫•t c·∫£ documents
        similarities = []
        for chunk_id, vector in self.vectors.items():
            similarity = self.enhanced_cosine_similarity(query_embedding, vector)
            if similarity >= similarity_threshold:  # L·ªçc k·∫øt qu·∫£ c√≥ similarity th·∫•p
                similarities.append((chunk_id, similarity))
        
        # S·∫Øp x·∫øp theo similarity (cao nh·∫•t tr∆∞·ªõc)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # L·∫•y k·∫øt qu·∫£ top n
        top_results = similarities[:n_results]
        
        print(f"‚úÖ T√¨m th·∫•y {len(top_results)} k·∫øt qu·∫£ ph√π h·ª£p (threshold: {similarity_threshold}):")
        
        results = []
        for i, (chunk_id, similarity) in enumerate(top_results):
            metadata = self.metadata[chunk_id]
            print(f"\n--- K·∫øt qu·∫£ {i+1} (similarity: {similarity:.4f}) ---")
            print(f"ID: {chunk_id}")
            print(f"Title: {metadata['title']}")
            print(f"Category: {metadata['category']}")
            print(f"Roles: {metadata['allowed_roles']}")
            print(f"Content: {metadata['content']}")
            
            results.append({
                'id': chunk_id,
                'similarity': similarity,
                'metadata': metadata
            })
        
        return results
    
    def semantic_search(self, query, n_results=5, boost_categories=None):
        """T√¨m ki·∫øm ng·ªØ nghƒ©a v·ªõi kh·∫£ nƒÉng boost categories"""
        if boost_categories is None:
            boost_categories = []
        
        results = self.search(query, n_results * 2, similarity_threshold=0.05)  # L·∫•y nhi·ªÅu k·∫øt qu·∫£ h∆°n
        
        # Boost k·∫øt qu·∫£ trong categories ƒë∆∞·ª£c ∆∞u ti√™n
        boosted_results = []
        normal_results = []
        
        for result in results:
            if result['metadata']['category'] in boost_categories:
                # TƒÉng similarity cho c√°c k·∫øt qu·∫£ trong category ƒë∆∞·ª£c boost
                result['similarity'] *= 1.2
                boosted_results.append(result)
            else:
                normal_results.append(result)
        
        # K·∫øt h·ª£p v√† s·∫Øp x·∫øp l·∫°i
        final_results = boosted_results + normal_results
        final_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        return final_results[:n_results]
    
    def save(self):
        """L∆∞u vector store"""
        import pickle
        
        data = {
            'vectors': self.vectors,
            'metadata': self.metadata,
            'vocab': self.vocab,
            'vocab_size': self.vocab_size,
            'vector_dim': self.vector_dim
        }
        
        with open(f'{self.persist_directory}/vector_store.pkl', 'wb') as f:
            pickle.dump(data, f)
        
        # L∆∞u vocabulary ri√™ng ƒë·ªÉ debug
        with open(f'{self.persist_directory}/vocab.json', 'w', encoding='utf-8') as f:
            json_vocab = {k: v for k, v in self.vocab.items()}
            json.dump(json_vocab, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ ƒê√£ l∆∞u vector store t·∫°i: {self.persist_directory}/vector_store.pkl")
    
    def load(self):
        """T·∫£i vector store"""
        import pickle
        
        try:
            with open(f'{self.persist_directory}/vector_store.pkl', 'rb') as f:
                data = pickle.load(f)
            
            self.vectors = data['vectors']
            self.metadata = data['metadata']
            self.vocab = data.get('vocab', {})
            self.vocab_size = data.get('vocab_size', 1000)
            self.vector_dim = data.get('vector_dim', 300)
            
            print(f"üìÇ ƒê√£ t·∫£i vector store v·ªõi {len(self.vectors)} documents")
            print(f"üìö Vocabulary size: {len(self.vocab)}")
            return True
        except FileNotFoundError:
            print("‚ÑπÔ∏è  Ch∆∞a c√≥ vector store ƒë∆∞·ª£c l∆∞u")
            return False

def main():
    print("üöÄ B·∫ÆT ƒê·∫¶U THI·∫æT L·∫¨P IMPROVED VECTOR STORE")
    print("=" * 50)
    
    # Kh·ªüi t·∫°o vector store c·∫£i ti·∫øn
    vector_store = ImprovedVectorStore()
    
    # Th·ª≠ t·∫£i vector store ƒë√£ l∆∞u
    if not vector_store.load():
        # N·∫øu ch∆∞a c√≥, t·∫°o m·ªõi t·ª´ chunks
        try:
            with open('outputs/document_chunks.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            chunks = data['chunks']
            print(f"üìñ ƒê√£ load {len(chunks)} chunks t·ª´ file")
            
            # Th√™m documents v√†o vector store
            vector_store.add_documents(chunks)
            
            # L∆∞u vector store
            vector_store.save()
            
        except FileNotFoundError:
            print("‚ùå File outputs/document_chunks.json kh√¥ng t·ªìn t·∫°i")
            print("   H√£y ch·∫°y B∆∞·ªõc 1.2 tr∆∞·ªõc")
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            sys.exit(1)
    
    # Test t√¨m ki·∫øm v·ªõi c√°c c·∫£i ti·∫øn
    print("\n" + "=" * 50)
    print("üß™ TEST T√åM KI·∫æM C·∫¢I TI·∫æN")
    print("=" * 50)
    
    # Test c∆° b·∫£n
    print("\n1. üîç T√åM KI·∫æM C∆† B·∫¢N:")
    vector_store.search("ngh·ªâ ph√©p nƒÉm", n_results=3)
    vector_store.search("l∆∞∆°ng th∆∞·ªüng h√†ng th√°ng", n_results=3)
    vector_store.search("gi·ªù l√†m vi·ªác c√¥ng ty", n_results=3)
    
    # Test semantic search v·ªõi boost category
    print("\n2. üîç T√åM KI·∫æM NG·ªÆ NGHƒ®A (BOOST CATEGORY):")
    semantic_results = vector_store.semantic_search(
        "b·∫£o hi·ªÉm", 
        n_results=3, 
        boost_categories=["B·∫£o Hi·ªÉm X√£ H·ªôi"]
    )
    
    for i, result in enumerate(semantic_results):
        print(f"\n--- Semantic Result {i+1} (similarity: {result['similarity']:.4f}) ---")
        print(f"Category: {result['metadata']['category']}")
        print(f"Title: {result['metadata']['title']}")
    
    # Test v·ªõi query ph·ª©c t·∫°p
    print("\n3. üîç T√åM KI·∫æM QUERY PH·ª®C T·∫†P:")
    vector_store.search("ch·∫ø ƒë·ªô ngh·ªâ ph√©p v√† l∆∞∆°ng th∆∞·ªüng", n_results=3)
    vector_store.search("ƒëi·ªÅu ki·ªán h∆∞·ªüng b·∫£o hi·ªÉm x√£ h·ªôi", n_results=3)
    
    print(f"\nüéâ HO√ÄN TH√ÄNH THI·∫æT L·∫¨P IMPROVED VECTOR STORE")
    print(f"üìÅ Vector store location: ./improved_vector_store")
    print(f"üìä Vector dimension: {vector_store.vector_dim}")
    print(f"üìö Vocabulary size: {len(vector_store.vocab)}")

if __name__ == "__main__":
    main()