# scripts/vector_store_fixed.py
import json
import os
import sys
import numpy as np
from collections import Counter
import re
import math

class FixedVectorStore:
    def __init__(self, persist_directory="./fixed_vector_store"):
        self.persist_directory = persist_directory
        os.makedirs(persist_directory, exist_ok=True)
        self.vectors = {}
        self.metadata = {}
        self.vector_dim = 100  # Gi·∫£m dimension ƒë·ªÉ tƒÉng m·∫≠t ƒë·ªô
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o Fixed Vector Store")
    
    def fixed_preprocess(self, text):
        """Ti·ªÅn x·ª≠ l√Ω ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£"""
        text = text.lower().strip()
        # Gi·ªØ l·∫°i c√°c k√Ω t·ª± quan tr·ªçng
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text
    
    def create_better_embedding(self, text):
        """T·∫°o embedding t·ªët h∆°n v·ªõi ph√¢n ph·ªëi ƒë·ªÅu"""
        processed_text = self.fixed_preprocess(text)
        words = processed_text.split()
        
        vector = np.zeros(self.vector_dim)
        
        if not words:
            return vector.tolist()
        
        # S·ª≠ d·ª•ng multiple hash functions ƒë·ªÉ ph√¢n ph·ªëi t·ªët h∆°n
        for word in words:
            if len(word) < 2:  # B·ªè qua t·ª´ qu√° ng·∫Øn
                continue
                
            # S·ª≠ d·ª•ng 3 hash functions kh√°c nhau
            for seed in range(3):
                hash_val = (hash(word + str(seed)) % (self.vector_dim // 3)) + (seed * (self.vector_dim // 3))
                vector[hash_val] += 1.0
        
        # Th√™m semantic boost cho t·ª´ kh√≥a quan tr·ªçng
        important_words = {
            'ngh·ªâ': 2.0, 'ph√©p': 2.0, 'ngh·ªâ ph√©p': 3.0,
            'l∆∞∆°ng': 2.0, 'th∆∞·ªüng': 2.0, 'l∆∞∆°ng th∆∞·ªüng': 3.0,
            'b·∫£o hi·ªÉm': 2.5, 'x√£ h·ªôi': 1.5,
            'gi·ªù': 2.0, 'l√†m vi·ªác': 2.5, 'gi·ªù l√†m': 3.0,
            'ch√≠nh s√°ch': 2.0, 'n·ªôi quy': 2.0, 'quy ƒë·ªãnh': 2.0,
            'c√¥ng ty': 1.5, 'nh√¢n vi√™n': 1.5, 'h·ª£p ƒë·ªìng': 2.0
        }
        
        for word, boost in important_words.items():
            if word in processed_text:
                # Boost c√°c dimensions li√™n quan
                for i in range(2):
                    boost_idx = (hash(word + f"boost_{i}") % self.vector_dim)
                    vector[boost_idx] += boost
        
        # Chu·∫©n h√≥a vector
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        else:
            # ƒê·∫£m b·∫£o vector kh√¥ng b·∫±ng 0
            vector = np.ones(self.vector_dim) / self.vector_dim
            
        return vector.tolist()
    
    def add_documents(self, chunks):
        """Th√™m documents v·ªõi embedding ƒë∆∞·ª£c c·∫£i thi·ªán"""
        print("üì• ƒêang th√™m documents v·ªõi fixed embedding...")
        
        for chunk in chunks:
            chunk_id = chunk['id']
            content = chunk['content']
            
            # T·∫°o embedding t·ªët h∆°n
            embedding = self.create_better_embedding(content)
            
            # L∆∞u th√¥ng tin
            self.vectors[chunk_id] = embedding
            self.metadata[chunk_id] = {
                "document_id": chunk['document_id'],
                "category": chunk['category'],
                "allowed_roles": chunk['allowed_roles'],
                "title": chunk['title'],
                "content": content[:200] + "..." if len(content) > 200 else content,
                "word_count": chunk['word_count']
            }
        
        print(f"‚úÖ ƒê√£ th√™m {len(chunks)} documents")
        
        # Hi·ªÉn th·ªã th·ªëng k√™
        categories = {}
        for chunk in chunks:
            cat = chunk['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"üìä Ph√¢n b·ªë theo category:")
        for cat, count in categories.items():
            print(f"   ‚Ä¢ {cat}: {count} chunks")
    
    def cosine_similarity(self, vec1, vec2):
        """Cosine similarity v·ªõi x·ª≠ l√Ω edge cases"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        
        # ƒê·∫£m b·∫£o similarity kh√¥ng √¢m
        return max(0.0, similarity)
    
    def search(self, query, n_results=5, similarity_threshold=0.1):
        """T√¨m ki·∫øm v·ªõi threshold th·∫•p h∆°n"""
        print(f"\nüîç T√åM KI·∫æM: '{query}'")
        
        # T·∫°o embedding cho query
        query_embedding = self.create_better_embedding(query)
        
        # T√≠nh similarity v·ªõi t·∫•t c·∫£ documents
        similarities = []
        for chunk_id, vector in self.vectors.items():
            similarity = self.cosine_similarity(query_embedding, vector)
            if similarity >= similarity_threshold:
                similarities.append((chunk_id, similarity))
        
        # S·∫Øp x·∫øp theo similarity (cao nh·∫•t tr∆∞·ªõc)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # L·∫•y k·∫øt qu·∫£ top n
        top_results = similarities[:n_results]
        
        print(f"‚úÖ T√¨m th·∫•y {len(top_results)} k·∫øt qu·∫£ (threshold: {similarity_threshold}):")
        
        for i, (chunk_id, similarity) in enumerate(top_results):
            metadata = self.metadata[chunk_id]
            print(f"\n--- K·∫øt qu·∫£ {i+1} (similarity: {similarity:.4f}) ---")
            print(f"Title: {metadata['title']}")
            print(f"Category: {metadata['category']}")
            print(f"Content: {metadata['content']}")
    
    def save(self):
        """L∆∞u vector store"""
        import pickle
        
        data = {
            'vectors': self.vectors,
            'metadata': self.metadata,
            'vector_dim': self.vector_dim
        }
        
        with open(f'{self.persist_directory}/vector_store.pkl', 'wb') as f:
            pickle.dump(data, f)
        
        print(f"üíæ ƒê√£ l∆∞u fixed vector store t·∫°i: {self.persist_directory}")
    
    def load(self):
        """T·∫£i vector store"""
        import pickle
        
        try:
            with open(f'{self.persist_directory}/vector_store.pkl', 'rb') as f:
                data = pickle.load(f)
            
            self.vectors = data['vectors']
            self.metadata = data['metadata']
            self.vector_dim = data.get('vector_dim', 100)
            
            print(f"üìÇ ƒê√£ t·∫£i fixed vector store v·ªõi {len(self.vectors)} documents")
            return True
        except FileNotFoundError:
            print("‚ÑπÔ∏è  Ch∆∞a c√≥ fixed vector store ƒë∆∞·ª£c l∆∞u")
            return False

def main():
    print("üöÄ T·∫†O FIXED VECTOR STORE V·ªöI EMBEDDING T·ªêT H∆†N")
    print("=" * 50)
    
    # Kh·ªüi t·∫°o vector store
    vector_store = FixedVectorStore()
    
    # Th·ª≠ t·∫£i vector store ƒë√£ l∆∞u
    if not vector_store.load():
        # N·∫øu ch∆∞a c√≥, t·∫°o m·ªõi t·ª´ chunks
        try:
            with open('outputs/document_chunks.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            chunks = data['chunks']
            print(f"üìñ ƒê√£ load {len(chunks)} chunks t·ª´ file")
            
            # Th√™m documents v√†o vector store
            vector_store.add_documents(chunks)
            
            # L∆∞u vector store
            vector_store.save()
            
        except FileNotFoundError:
            print("‚ùå File outputs/document_chunks.json kh√¥ng t·ªìn t·∫°i")
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            sys.exit(1)
    
    # Test t√¨m ki·∫øm v·ªõi threshold th·∫•p
    print("\n" + "=" * 50)
    print("üß™ TEST T√åM KI·∫æM V·ªöI FIXED EMBEDDING")
    print("=" * 50)
    
    # Test v·ªõi threshold th·∫•p h∆°n
    test_queries = [
        "ngh·ªâ ph√©p",
        "l∆∞∆°ng th∆∞·ªüng", 
        "b·∫£o hi·ªÉm x√£ h·ªôi",
        "gi·ªù l√†m vi·ªác",
        "n·ªôi quy c√¥ng ty"
    ]
    
    for query in test_queries:
        vector_store.search(query, n_results=3, similarity_threshold=0.05)
    
    print(f"\nüéâ HO√ÄN TH√ÄNH FIXED VECTOR STORE")
    print(f"üìÅ Vector store location: ./fixed_vector_store")
    print(f"üìä Vector dimension: {vector_store.vector_dim}")

if __name__ == "__main__":
    main()