# scripts/document_processor.py
import os
import json
import sys
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # K√≠ch th∆∞·ªõc m·ªói chunk
            chunk_overlap=200,    # ƒê·ªô ch·ªìng l·∫•p gi·ªØa c√°c chunk
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
    
    def extract_text_from_file(self, file_path):
        """Extract text t·ª´ nhi·ªÅu ƒë·ªãnh d·∫°ng file"""
        file_extension = Path(file_path).suffix.lower()
        
        try:
            if file_extension == '.pdf':
                return self._extract_from_pdf(file_path)
            elif file_extension in ['.docx', '.doc']:
                return self._extract_from_docx(file_path)
            elif file_extension in ['.txt', '.md']:
                return self._extract_from_text(file_path)
            else:
                print(f"‚ö†Ô∏è  ƒê·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£: {file_extension}")
                return ""
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file {file_path}: {e}")
            return ""
    
    def _extract_from_pdf(self, file_path):
        """Extract text t·ª´ PDF"""
        try:
            from PyPDF2 import PdfReader
            
            print(f"   üìÑ ƒê·ªçc PDF: {file_path}")
            reader = PdfReader(file_path)
            text = ""
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                print(f"     üìñ ƒê√£ x·ª≠ l√Ω trang {i+1}/{len(reader.pages)}")
            return text
        except ImportError:
            print("‚ùå PyPDF2 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            return ""
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc PDF {file_path}: {e}")
            return ""
    
    def _extract_from_docx(self, file_path):
        """Extract text t·ª´ DOCX"""
        try:
            from docx import Document
            
            print(f"   üìÑ ƒê·ªçc DOCX: {file_path}")
            doc = Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text += paragraph.text + "\n"
            return text
        except ImportError:
            print("‚ùå python-docx ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
            return ""
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc DOCX {file_path}: {e}")
            return ""
    
    def _extract_from_text(self, file_path):
        """Extract text t·ª´ TXT/MD"""
        try:
            print(f"   üìÑ ƒê·ªçc text file: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    return f.read()
            except Exception as e:
                print(f"‚ùå L·ªói encoding file {file_path}: {e}")
                return ""
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file {file_path}: {e}")
            return ""
    
    def clean_text(self, text):
        """L√†m s·∫°ch text"""
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = ' '.join(text.split())
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng in ƒë∆∞·ª£c nh∆∞ng gi·ªØ ti·∫øng Vi·ªát
        text = ''.join(char for char in text if char.isprintable() or char in ['\n', '\t', ' '])
        return text
    
    def process_documents(self, metadata_file, output_file):
        """X·ª≠ l√Ω t·∫•t c·∫£ documents v√† t·∫°o chunks"""
        print("üìñ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω documents...")
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c output t·ªìn t·∫°i
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Load metadata
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except FileNotFoundError:
            print(f"‚ùå File metadata kh√¥ng t·ªìn t·∫°i: {metadata_file}")
            return None
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói ƒë·ªãnh d·∫°ng JSON trong file metadata: {e}")
            return None
        
        all_chunks = []
        processed_count = 0
        error_count = 0
        
        for doc_meta in metadata['documents']:
            file_path = doc_meta['file_path']
            print(f"\nüîç ƒêang x·ª≠ l√Ω: {file_path}")
            
            if not os.path.exists(file_path):
                print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
                error_count += 1
                continue
            
            # Extract text
            raw_text = self.extract_text_from_file(file_path)
            
            if not raw_text.strip():
                print(f"‚ö†Ô∏è  File r·ªóng ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {file_path}")
                error_count += 1
                continue
            
            # Clean text
            cleaned_text = self.clean_text(raw_text)
            
            # Split th√†nh chunks
            try:
                chunks = self.text_splitter.split_text(cleaned_text)
                print(f"   ‚úÖ ƒê√£ chia th√†nh {len(chunks)} chunks")
            except Exception as e:
                print(f"‚ùå L·ªói khi split text: {e}")
                error_count += 1
                continue
            
            # Th√™m metadata v√†o t·ª´ng chunk
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "id": f"{doc_meta['id']}_chunk_{i:03d}",
                    "content": chunk,
                    "document_id": doc_meta['id'],
                    "category": doc_meta['category'],
                    "allowed_roles": doc_meta['allowed_roles'],
                    "title": doc_meta['title'],
                    "description": doc_meta.get('description', ''),
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "file_path": file_path,
                    "word_count": len(chunk.split())
                }
                all_chunks.append(chunk_data)
            
            processed_count += 1
        
        # L∆∞u k·∫øt qu·∫£
        output_data = {
            "statistics": {
                "total_documents": len(metadata['documents']),
                "processed_documents": processed_count,
                "error_documents": error_count,
                "total_chunks": len(all_chunks),
                "average_chunks_per_doc": len(all_chunks) / processed_count if processed_count > 0 else 0
            },
            "chunks": all_chunks
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nüìä TH·ªêNG K√ä X·ª¨ L√ù:")
        print(f"   ‚Ä¢ T·ªïng documents: {len(metadata['documents'])}")
        print(f"   ‚Ä¢ X·ª≠ l√Ω th√†nh c√¥ng: {processed_count}")
        print(f"   ‚Ä¢ L·ªói: {error_count}")
        print(f"   ‚Ä¢ T·ªïng chunks: {len(all_chunks)}")
        print(f"   ‚Ä¢ File output: {output_file}")
        
        return output_data

def main():
    processor = DocumentProcessor()
    
    # X·ª≠ l√Ω documents
    result = processor.process_documents(
        metadata_file='config/documents_metadata.json',
        output_file='outputs/document_chunks.json'
    )
    
    if result and result['chunks']:
        # Hi·ªÉn th·ªã sample chunks
        print(f"\nüìù SAMPLE CHUNKS:")
        for i, chunk in enumerate(result['chunks'][:2]):  # Hi·ªÉn th·ªã 2 chunks ƒë·∫ßu
            print(f"\n--- Chunk {i+1} ---")
            print(f"ID: {chunk['id']}")
            print(f"Title: {chunk['title']}")
            print(f"Content preview: {chunk['content'][:100]}...")
            print(f"Word count: {chunk['word_count']}")
    else:
        print("‚ùå Kh√¥ng c√≥ chunks n√†o ƒë∆∞·ª£c t·∫°o ra")

if __name__ == "__main__":
    main()