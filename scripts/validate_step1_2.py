# scripts/validate_step1_2.py
import os
import json
import sys

def validate_step1_2():
    print("üîç KI·ªÇM TRA HO√ÄN TH√ÄNH B∆Ø·ªöC 1.2")
    print("=" * 50)
    
    # Ki·ªÉm tra dependencies
    print("1. Ki·ªÉm tra dependencies:")
    required_packages = [
        "PyPDF2", "docx", "langchain", 
        "langchain_text_splitters", "sentence_transformers"
    ]
    
    deps_ok = True
    for package in required_packages:
        try:
            __import__(package)
            print(f"   ‚úÖ {package}")
        except ImportError:
            print(f"   ‚ùå {package} - CH∆ØA C√ÄI ƒê·∫∂T")
            deps_ok = False
    
    # Ki·ªÉm tra file output
    print("\n2. Ki·ªÉm tra k·∫øt qu·∫£ x·ª≠ l√Ω:")
    output_file = 'outputs/document_chunks.json'
    
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            stats = data.get('statistics', {})
            chunks = data.get('chunks', [])
            
            print(f"   ‚úÖ {output_file}")
            print(f"   ‚Ä¢ Documents processed: {stats.get('processed_documents', 0)}")
            print(f"   ‚Ä¢ Total chunks: {stats.get('total_chunks', 0)}")
            print(f"   ‚Ä¢ Error documents: {stats.get('error_documents', 0)}")
            
            # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng chunks
            if chunks:
                sample_chunk = chunks[0]
                print(f"   ‚Ä¢ Sample chunk ID: {sample_chunk.get('id')}")
                print(f"   ‚Ä¢ Sample word count: {sample_chunk.get('word_count')}")
                
        except Exception as e:
            print(f"   ‚ùå {output_file} - L·ªñI: {e}")
            deps_ok = False
    else:
        print(f"   ‚ùå {output_file} - CH∆ØA ƒê∆Ø·ª¢C T·∫†O")
        deps_ok = False
    
    # Ki·ªÉm tra chunks quality
    print("\n3. Ki·ªÉm tra ch·∫•t l∆∞·ª£ng chunks:")
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = data.get('chunks', [])
        if chunks:
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc chunks
            chunk_sizes = [len(chunk['content'].split()) for chunk in chunks[:10]]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            
            print(f"   ‚Ä¢ Avg words per chunk: {avg_size:.1f}")
            
            if 50 < avg_size < 150:  # Kho·∫£ng h·ª£p l√Ω
                print(f"   ‚úÖ K√≠ch th∆∞·ªõc chunks ph√π h·ª£p")
            else:
                print(f"   ‚ö†Ô∏è  K√≠ch th∆∞·ªõc chunks c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u")
            
            # Ki·ªÉm tra metadata
            first_chunk = chunks[0]
            required_fields = ['id', 'content', 'category', 'allowed_roles', 'title']
            missing_fields = [field for field in required_fields if field not in first_chunk]
            
            if not missing_fields:
                print(f"   ‚úÖ Metadata ƒë·∫ßy ƒë·ªß")
            else:
                print(f"   ‚ùå Thi·∫øu fields: {missing_fields}")
                deps_ok = False
    
    # T·ªïng k·∫øt
    print("\n" + "=" * 50)
    if deps_ok:
        print("üéâ HO√ÄN TH√ÄNH B∆Ø·ªöC 1.2 - CHUY·ªÇN ƒê·ªîI TEXT TH√ÄNH C√îNG")
        print("\nüìä K·∫æT QU·∫¢:")
        print(f"   ‚Ä¢ Documents ƒë√£ x·ª≠ l√Ω: {stats.get('processed_documents')}")
        print(f"   ‚Ä¢ T·ªïng s·ªë chunks: {stats.get('total_chunks')}")
        print(f"   ‚Ä¢ Dependencies: ƒê·∫ßy ƒë·ªß")
        return True
    else:
        print("‚ùå CH∆ØA HO√ÄN TH√ÄNH - Vui l√≤ng ki·ªÉm tra v√† th·ª≠ l·∫°i")
        return False

if __name__ == "__main__":
    success = validate_step1_2()
    sys.exit(0 if success else 1)