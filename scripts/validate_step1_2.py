# scripts/validate_step1_2_fixed.py
import os
import json
import sys

def validate_step1_2():
    print("üîç KI·ªÇM TRA HO√ÄN TH√ÄNH B∆Ø·ªöC 1.2")
    print("=" * 50)
    
    # Ki·ªÉm tra dependencies - ch·ªâ nh·ªØng package th·ª±c s·ª± c·∫ßn thi·∫øt
    print("1. Ki·ªÉm tra dependencies:")
    required_packages = [
        "PyPDF2", "docx", "langchain_text_splitters"
    ]
    
    optional_packages = [
        "langchain", "sentence_transformers"
    ]
    
    deps_ok = True
    for package in required_packages:
        try:
            __import__(package)
            print(f"   ‚úÖ {package}")
        except ImportError:
            print(f"   ‚ùå {package} - CH∆ØA C√ÄI ƒê·∫∂T")
            deps_ok = False
    
    print("\n   Package t√πy ch·ªçn:")
    for package in optional_packages:
        try:
            __import__(package)
            print(f"   ‚úÖ {package} (optional)")
        except ImportError:
            print(f"   ‚ö†Ô∏è  {package} - Ch∆∞a c√†i ƒë·∫∑t (kh√¥ng b·∫Øt bu·ªôc)")
    
    # Ki·ªÉm tra file output
    print("\n2. Ki·ªÉm tra k·∫øt qu·∫£ x·ª≠ l√Ω:")
    output_file = 'outputs/document_chunks.json'
    
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            stats = data.get('statistics', {})
            chunks = data.get('chunks', [])
            
            print(f"   ‚úÖ {output_file}")
            print(f"   ‚Ä¢ Documents processed: {stats.get('processed_documents', 0)}")
            print(f"   ‚Ä¢ Total chunks: {stats.get('total_chunks', 0)}")
            print(f"   ‚Ä¢ Error documents: {stats.get('error_documents', 0)}")
            
            # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng chunks
            if chunks:
                sample_chunk = chunks[0]
                print(f"   ‚Ä¢ Sample chunk ID: {sample_chunk.get('id')}")
                print(f"   ‚Ä¢ Sample word count: {sample_chunk.get('word_count')}")
                
        except Exception as e:
            print(f"   ‚ùå {output_file} - L·ªñI: {e}")
            deps_ok = False
    else:
        print(f"   ‚ùå {output_file} - CH∆ØA ƒê∆Ø·ª¢C T·∫†O")
        deps_ok = False
    
    # Ki·ªÉm tra chunks quality
    print("\n3. Ki·ªÉm tra ch·∫•t l∆∞·ª£ng chunks:")
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        chunks = data.get('chunks', [])
        if chunks:
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc chunks
            chunk_sizes = [len(chunk['content'].split()) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            min_size = min(chunk_sizes)
            max_size = max(chunk_sizes)
            
            print(f"   ‚Ä¢ S·ªë chunks: {len(chunks)}")
            print(f"   ‚Ä¢ T·ª´/chunk (trung b√¨nh): {avg_size:.1f}")
            print(f"   ‚Ä¢ T·ª´/chunk (min-max): {min_size}-{max_size}")
            
            # ƒê√°nh gi√° k√≠ch th∆∞·ªõc
            if 50 < avg_size < 500:  # Kho·∫£ng h·ª£p l√Ω r·ªông h∆°n
                print(f"   ‚úÖ K√≠ch th∆∞·ªõc chunks ph√π h·ª£p")
            else:
                print(f"   ‚ö†Ô∏è  K√≠ch th∆∞·ªõc chunks c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u")
            
            # Ki·ªÉm tra metadata
            first_chunk = chunks[0]
            required_fields = ['id', 'content', 'category', 'allowed_roles', 'title']
            missing_fields = [field for field in required_fields if field not in first_chunk]
            
            if not missing_fields:
                print(f"   ‚úÖ Metadata ƒë·∫ßy ƒë·ªß")
            else:
                print(f"   ‚ùå Thi·∫øu fields: {missing_fields}")
                deps_ok = False
            
            # Ki·ªÉm tra content kh√¥ng r·ªóng
            empty_chunks = [chunk for chunk in chunks if not chunk['content'].strip()]
            if not empty_chunks:
                print(f"   ‚úÖ Kh√¥ng c√≥ chunks r·ªóng")
            else:
                print(f"   ‚ùå C√≥ {len(empty_chunks)} chunks r·ªóng")
                deps_ok = False
    
    # Ki·ªÉm tra file metadata
    print("\n4. Ki·ªÉm tra file metadata:")
    metadata_file = 'config/documents_metadata.json'
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"   ‚úÖ {metadata_file}")
            print(f"   ‚Ä¢ S·ªë documents: {len(metadata.get('documents', []))}")
        except Exception as e:
            print(f"   ‚ùå {metadata_file} - L·ªñI: {e}")
            deps_ok = False
    else:
        print(f"   ‚ùå {metadata_file} - KH√îNG T·ªíN T·∫†I")
        deps_ok = False
    
    # T·ªïng k·∫øt
    print("\n" + "=" * 50)
    if deps_ok:
        print("üéâ HO√ÄN TH√ÄNH B∆Ø·ªöC 1.2 - CHUY·ªÇN ƒê·ªîI TEXT TH√ÄNH C√îNG")
        print("\nüìä K·∫æT QU·∫¢:")
        print(f"   ‚Ä¢ Documents ƒë√£ x·ª≠ l√Ω: {stats.get('processed_documents')}")
        print(f"   ‚Ä¢ T·ªïng s·ªë chunks: {stats.get('total_chunks')}")
        print(f"   ‚Ä¢ Dependencies: ƒê·∫ßy ƒë·ªß")
        print(f"   ‚Ä¢ File output: {output_file}")
        return True
    else:
        print("‚ùå CH∆ØA HO√ÄN TH√ÄNH - Vui l√≤ng ki·ªÉm tra v√† th·ª≠ l·∫°i")
        return False

if __name__ == "__main__":
    success = validate_step1_2()
    sys.exit(0 if success else 1)